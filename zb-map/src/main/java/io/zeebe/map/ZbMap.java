/*
 * Copyright Â© 2017 camunda services GmbH (info@camunda.com)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package io.zeebe.map;

import static io.zeebe.map.BucketBufferArrayDescriptor.BUCKET_DATA_OFFSET;

import io.zeebe.util.StringUtil;
import java.lang.reflect.ParameterizedType;
import java.util.HashSet;
import java.util.Set;
import java.util.concurrent.atomic.AtomicBoolean;
import org.agrona.BitUtil;
import org.agrona.CloseHelper;
import org.slf4j.Logger;

/**
 * Simple map data structure using extensible hashing. Data structure is not threadsafe.
 *
 * <p>The map size should be a power of two. If it is not a power of two the next power of two is
 * used. The max map size is {@link ZbMap#MAX_TABLE_SIZE} as the map stores long addresses and this
 * is the maximum number of entries which can be addressed with map keys generated by {@link
 * KeyHandler#keyHashCode}.
 */
public abstract class ZbMap<K extends KeyHandler, V extends ValueHandler> {
  private static final int KEY_HANDLER_IDX = 0;
  private static final int VALUE_HANDLER_IDX = 1;

  /**
   * The shrink limit which is used to indicate, whether the hash table should be shrinked or not,
   * see {@link #tryShrinkHashTable()}.
   */
  private static final float HASH_TABLE_SHRINK_LIMIT = 0.25F;

  /** Default hash table start size. */
  public static final int DEFAULT_TABLE_SIZE = 32;

  /**
   * The default block count of 16 is the optimal block count regarding to performance and memory
   * usage. Was determined with help of some benchmarks see {@link
   * io.zeebe.map.benchmarks.ZbMapDetermineSizesBenchmark}.
   */
  public static final int DEFAULT_BLOCK_COUNT = 16;

  private static final String FINALIZER_WARNING =
      "ZbMap {} is being garbage collected but is not closed.\n"
          + "This means that the object is being de-referenced but the close() method has not been called.\n"
          + "ZbMap allocates memory off the heap which is not reclaimed unless close() is invoked.\n";

  public static final Logger LOG = Loggers.ZB_MAP_LOGGER;

  /**
   * The maximum table size is 2^27, because it is the last power of two which fits into an integer
   * after multiply with SIZE_OF_LONG (8 bytes).
   *
   * <p>The table size have to be multiplied with SIZE_OF_LONG to calculated the size of the hash
   * table buffer, which have to be allocated to store all addresses. The addresses, which are
   * stored in the hash table buffer, are longs.
   */
  public static final int MAX_TABLE_SIZE = 1 << 27;

  protected final K keyHandler;
  protected final K splitKeyHandler;
  protected final V valueHandler;

  protected final HashTable hashTable;
  protected final BucketBufferArray bucketBufferArray;

  protected int maxTableSize;
  protected final int initialTableSize;

  protected ZbMapBucketMergeHelper bucketMergeHelper;

  /**
   * The number of times this HashMap has been structurally modified. Structural modifications are
   * those that change the number of mappings in the HashMap or otherwise modify its internal
   * structure (e.g., rehash). This field is used to make iterators fail-fast. (See
   * ConcurrentModificationException).
   */
  protected int modCount;

  private final Block blockHelperInstance = new Block();
  protected final AtomicBoolean isClosed = new AtomicBoolean(false);

  public ZbMap(int maxKeyLength, int maxValueLength) {
    this(DEFAULT_TABLE_SIZE, DEFAULT_BLOCK_COUNT, maxKeyLength, maxValueLength);
  }

  /**
   * Creates an hash map object.
   *
   * <p>The map will grow dynamically. After storing `X` entries, which is equal to `{@link
   * #MAX_TABLE_SIZE} * maxBlockLength`, the hash table can't grow further, then overflow will be
   * used as fallback. The maxBlockLength is equal to {@link
   * BucketBufferArrayDescriptor#BUCKET_DATA_OFFSET} + (bucketCount * {@link
   * BucketBufferArrayDescriptor#getBlockLength(int, int)}))
   *
   * <p>Note: it could happen that some hashes modulo the map size generate the same bucket id,
   * which means some buckets can be filled more than other. The buckets will be tried to splitted
   * in that case. Is the new generated bucket id larger than the table size the table will be
   * dynamically resized (if the next size is smaller then the maximum size). If the current load
   * factor is less then the maximum load factor the bucket will overflow before the table size is
   * increased. To avoid this the implementation of the `KeyHandler` have to provide a good one way
   * function, so the entries have a good distribution. <b>Example:</b>
   *
   * <pre>
   * map with map size 6 and maxBlockLength of 3 * (VAL len + KEY len)
   * KeyTable                Buckets:                                      Overflow
   * [bucket0]      ->     [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]  -> [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ] -> [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]
   * [bucket1]      ->     [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]
   * [bucket2]      ->     [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]
   * [bucket3]      ->     [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]
   * [bucket4]      ->     [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]
   * [bucket5]      ->     [ [KEY | VAL] | [KEY | VAL] | [KEY | VAL] ]
   * </pre>
   *
   * @param initialTableSize is the count of buckets, which should been used initial by the hash map
   * @param minBlockCount the minimal count of blocks which should fit in a bucket
   * @param maxKeyLength the max length of a key
   * @param maxValueLength the max length of a value
   */
  public ZbMap(int initialTableSize, int minBlockCount, int maxKeyLength, int maxValueLength) {
    if (LOG.isTraceEnabled()) {
      LOG.trace(
          "Creating map {} in context: \n{}",
          System.identityHashCode(this),
          StringUtil.formatStackTrace(Thread.currentThread().getStackTrace()));
    }

    this.keyHandler = createKeyHandlerInstance(maxKeyLength);
    this.splitKeyHandler = createKeyHandlerInstance(maxKeyLength);
    this.valueHandler = createValueHandlerInstance(maxValueLength);

    this.maxTableSize = MAX_TABLE_SIZE;
    this.initialTableSize = ensureTableSizeIsPowerOfTwo(initialTableSize);

    this.hashTable = new HashTable(this.initialTableSize);
    this.bucketBufferArray = new BucketBufferArray(minBlockCount, maxKeyLength, maxValueLength);
    this.bucketMergeHelper =
        new ZbMapBucketMergeHelper(bucketBufferArray, hashTable, minBlockCount);

    init();
  }

  public long getHashTableSize() {
    return hashTable.getLength();
  }

  public long size() {
    return hashTable.serializationSize() + bucketBufferArray.size();
  }

  private int ensureTableSizeIsPowerOfTwo(final int tableSize) {
    final int powerOfTwo = BitUtil.findNextPositivePowerOfTwo(tableSize);

    if (powerOfTwo != tableSize) {
      LOG.warn(
          "Supplied hash table size {} is not a power of two. Using next power of two {} instead.",
          tableSize,
          powerOfTwo);
    }

    if (powerOfTwo > MAX_TABLE_SIZE) {
      LOG.warn(
          "Size {} greater then max hash table size. Using max hash table size {} instead.",
          powerOfTwo,
          MAX_TABLE_SIZE);
      return MAX_TABLE_SIZE;
    } else {
      return powerOfTwo;
    }
  }

  private void init() {
    final long bucketAddress = this.bucketBufferArray.allocateNewBucket(0, 0);
    for (int idx = 0; idx < hashTable.getCapacity(); idx++) {
      hashTable.setBucketAddress(idx, bucketAddress);
    }

    modCount = 0;
  }

  public void close() {
    if (isClosed.compareAndSet(false, true)) {
      CloseHelper.quietClose(hashTable);
      CloseHelper.quietClose(bucketBufferArray);
    }
  }

  @Override
  protected void finalize() throws Throwable {
    if (!isClosed.get()) {
      LOG.error(FINALIZER_WARNING, System.identityHashCode(this));
    }
  }

  public void clear() {
    hashTable.clear();
    bucketBufferArray.clear();

    init();
  }

  private K createKeyHandlerInstance(int maxKeyLength) {
    final K keyHandler = createInstance(KEY_HANDLER_IDX);
    keyHandler.setKeyLength(maxKeyLength);
    return keyHandler;
  }

  private V createValueHandlerInstance(int maxValueHandlerLength) {
    final V valueHandler = createInstance(VALUE_HANDLER_IDX);
    valueHandler.setValueLength(maxValueHandlerLength);
    return valueHandler;
  }

  @SuppressWarnings("unchecked")
  private <T> T createInstance(int map) {
    Class<T> tClass = null;
    try {
      tClass =
          (Class<T>)
              ((ParameterizedType) this.getClass().getGenericSuperclass())
                  .getActualTypeArguments()[map];
      return tClass.newInstance();
    } catch (InstantiationException | IllegalAccessException e) {
      throw new RuntimeException("Could not instantiate " + tClass, e);
    }
  }

  protected void setMaxTableSize(int maxTableSize) {
    this.maxTableSize = ensureTableSizeIsPowerOfTwo(maxTableSize);
  }

  public int bucketCount() {
    return bucketBufferArray.getBucketCount();
  }

  protected boolean put() {
    final int keyHashCode = keyHandler.keyHashCode();
    int bucketId = getBucketId(keyHashCode);
    boolean isUpdated = false;
    boolean isPut = false;
    boolean scanForKey = true;

    while (!isPut && !isUpdated) {
      long bucketAddress = hashTable.getBucketAddress(bucketId);

      if (scanForKey) {
        final Block block = findBlockInBucket(bucketAddress);
        final boolean blockWasFound = block.wasFound();
        if (blockWasFound) {
          bucketAddress = block.getBucketAddress();
          final int blockOffset = block.getBlockOffset();
          bucketBufferArray.updateValue(valueHandler, bucketAddress, blockOffset);
          modCount += 1;
          isUpdated = true;
        }
        scanForKey = blockWasFound;
      } else {
        isPut = bucketBufferArray.addBlock(bucketAddress, keyHandler, valueHandler);

        if (!isPut) {
          splitBucket(bucketAddress);
          bucketId = getBucketId(keyHashCode);
        }

        modCount += 1;
      }
    }
    return isUpdated;
  }

  private int getBucketId(int keyHashCode) {
    final int bucketId;
    final int mask = hashTable.getCapacity() - 1;
    bucketId = (keyHashCode & mask);
    return bucketId;
  }

  protected boolean remove() {
    final Block block = findBlock();
    final boolean wasFound = block.wasFound();
    if (wasFound) {
      final long bucketAddress = block.getBucketAddress();
      final int blockOffset = block.getBlockOffset();
      bucketBufferArray.readValue(valueHandler, bucketAddress, blockOffset);
      final int newBucketFillCount = bucketBufferArray.removeBlock(bucketAddress, blockOffset);

      bucketMergeHelper.tryMergingBuckets(bucketAddress, newBucketFillCount);
      tryShrinkHashTable();
      modCount += 1;
    }
    return wasFound;
  }

  /**
   * Tries to shrink the hash table.
   *
   * <p>If the highest bucket id is below the {@link #HASH_TABLE_SHRINK_LIMIT} multiplied with the
   * hash table capacity, then the hash table will be shrinked.
   */
  private void tryShrinkHashTable() {
    final float shrinkLimit = hashTable.getCapacity() * HASH_TABLE_SHRINK_LIMIT;
    final int minimalCapacity = bucketBufferArray.getHighestBucketId() + 1;

    if (minimalCapacity <= shrinkLimit && hashTable.getCapacity() != initialTableSize) {
      if (minimalCapacity < initialTableSize && hashTable.getCapacity() > initialTableSize) {
        hashTable.resize(initialTableSize);
      } else {
        hashTable.resize(minimalCapacity);
      }
    }
  }

  protected boolean get() {
    final Block block = findBlock();
    final boolean wasFound = block.wasFound();
    if (wasFound) {
      bucketBufferArray.readValue(valueHandler, block.getBucketAddress(), block.getBlockOffset());
    }
    return wasFound;
  }

  private Block findBlock() {
    final int keyHashCode = keyHandler.keyHashCode();
    final int bucketId = getBucketId(keyHashCode);
    final long bucketAddress = hashTable.getBucketAddress(bucketId);
    return findBlockInBucket(bucketAddress);
  }

  private Block findBlockInBucket(long bucketAddress) {
    final Block foundBlock = blockHelperInstance;
    foundBlock.reset();
    boolean keyFound = false;

    do {
      final int bucketFillCount = bucketBufferArray.getBucketFillCount(bucketAddress);
      int blockOffset = bucketBufferArray.getFirstBlockOffset();
      int blocksVisited = 0;

      while (!keyFound && blocksVisited < bucketFillCount) {
        keyFound = bucketBufferArray.keyEquals(keyHandler, bucketAddress, blockOffset);

        if (keyFound) {
          foundBlock.set(bucketAddress, blockOffset);
        }

        blockOffset += bucketBufferArray.getBlockLength();
        blocksVisited++;
      }

      bucketAddress = bucketBufferArray.getBucketOverflowPointer(bucketAddress);
    } while (!keyFound && bucketAddress > 0);
    return foundBlock;
  }

  /** splits a block performing the map update and relocation and compaction of blocks. */
  private void splitBucket(long filledBucketAddress) {
    final int filledBucketId = bucketBufferArray.getBucketId(filledBucketAddress);
    final int bucketDepth = bucketBufferArray.getBucketDepth(filledBucketAddress);

    // calculate new ids and depths
    final int newBucketId = 1 << bucketDepth | filledBucketId;
    final int newBucketDepth = bucketDepth + 1;

    if (newBucketId < hashTable.getCapacity()) {
      createNewBucket(filledBucketAddress, bucketDepth, newBucketId, newBucketDepth);
    } else {

      final int newTableSize = hashTable.getCapacity() << 1;
      if (newTableSize > maxTableSize) {
        bucketBufferArray.overflow(filledBucketAddress);
      } else {
        hashTable.resize(newTableSize);
        createNewBucket(filledBucketAddress, bucketDepth, newBucketId, newBucketDepth);
      }
    }
  }

  private void createNewBucket(
      long filledBucketAddress, int bucketDepth, int newBucketId, int newBucketDepth) {
    // update filled block depth
    bucketBufferArray.setBucketDepth(filledBucketAddress, newBucketDepth);

    // create new bucket
    final long newBucketAddress = bucketBufferArray.allocateNewBucket(newBucketId, newBucketDepth);

    // distribute entries into correct blocks
    distributeEntries(filledBucketAddress, newBucketAddress, bucketDepth);

    // update map
    hashTable.updateTable(newBucketDepth, newBucketId, newBucketAddress);
  }

  private void distributeEntries(long filledBucketAddress, long newBucketAddress, int bucketDepth) {
    do {
      final int bucketFillCount = bucketBufferArray.getBucketFillCount(filledBucketAddress);
      final int splitMask = 1 << bucketDepth;

      int blockOffset = BUCKET_DATA_OFFSET;
      int blocksVisited = 0;

      while (blocksVisited < bucketFillCount) {
        final int blockLength = bucketBufferArray.getBlockLength();

        bucketBufferArray.readKey(splitKeyHandler, filledBucketAddress, blockOffset);
        final int keyHashCode = splitKeyHandler.keyHashCode();

        if ((keyHashCode & splitMask) == splitMask) {
          bucketBufferArray.relocateBlock(filledBucketAddress, blockOffset, newBucketAddress);
        } else {
          blockOffset += blockLength;
        }

        blocksVisited++;
      }
      filledBucketAddress = bucketBufferArray.getBucketOverflowPointer(filledBucketAddress);
    } while (filledBucketAddress != 0);
  }

  public BucketBufferArray getBucketBufferArray() {
    return bucketBufferArray;
  }

  public HashTable getHashTable() {
    return hashTable;
  }

  private static class Block {
    private long bucketAddress;
    private int blockOffset;

    public void reset() {
      bucketAddress = -1;
      blockOffset = -1;
    }

    public boolean wasFound() {
      return bucketAddress != -1 && blockOffset != -1;
    }

    public void set(long bucketAddress, int blockOffset) {
      this.bucketAddress = bucketAddress;
      this.blockOffset = blockOffset;
    }

    public long getBucketAddress() {
      return bucketAddress;
    }

    public int getBlockOffset() {
      return blockOffset;
    }
  }

  public String toString() {
    final StringBuilder builder = new StringBuilder();
    builder
        .append("Map:[ Buckets:")
        .append(bucketBufferArray.getBucketCount())
        .append(", Blocks: ")
        .append(bucketBufferArray.getBlockCount())
        .append("], Buckets:[");

    final long blockCount[] = new long[10];
    final long overflowCount[] = new long[10];
    final Set<Long> addresses = new HashSet<>();
    int count = 0;
    final int tableSize = hashTable.getCapacity();
    for (int i = 0; i < tableSize; i++) {
      final int idx = (int) (((double) i / (double) tableSize) * 10);

      final long bucketAddress = hashTable.getBucketAddress(i);
      if (!addresses.contains(bucketAddress)) {
        blockCount[idx] += bucketBufferArray.getBucketFillCount(bucketAddress);
        overflowCount[idx] += bucketBufferArray.getBucketOverflowCount(bucketAddress);
        addresses.add(bucketAddress);
      } else {
        count++;
      }
    }

    for (int i = 0; i < 10; i++) {
      builder
          .append("\n")
          .append(i * 10)
          .append("% ")
          .append(blockCount[i])
          .append(" blocks, is ")
          .append(((double) blockCount[i] / (double) bucketBufferArray.getBlockCount()) * 100)
          .append(" % of all. Have ")
          .append(overflowCount[i])
          .append(" overflows");
    }
    builder
        .append("\n], ")
        .append(count)
        .append(" indices point to the same bucket.")
        .append(" Table size: ")
        .append(tableSize);
    return builder.toString();
  }
}
